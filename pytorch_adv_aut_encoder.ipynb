{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "seed = 10\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "n_classes = 10\n",
    "z_dim = 2\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "train_batch_size =16#args.batch_size\n",
    "valid_batch_size = 16#.batch_size\n",
    "N = 1000\n",
    "epochs = 10#args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        self.lin1=nn.Linear(X_dim,N)\n",
    "        self.lin2=nn.Linear(N,N)\n",
    "        self.lin3gauss=nn.Linear(N,z_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.droppout(self.lin1(x),p=0.25,training=self.training)\n",
    "        x=F.relu(x)\n",
    "        x=F.droppout(self.lin2(x),p=0.25,training=self.training)\n",
    "        x=F.relu(x)\n",
    "        xgauss=self.lin3gauss(x)\n",
    "        return xgauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class P_net(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(P_net, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, X_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class D_net_gauss(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(D_net_gauss, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, N)\n",
    "        self.lin2 = nn.Linear(N, N)\n",
    "        self.lin3 = nn.Linear(N, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        return F.sigmoid(self.lin3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(10)  \n",
    "Q, P = Q_net(), P_net()     # Encoder/Decoder  \n",
    "D_gauss = D_net_gauss()                # Discriminator adversarial  \n",
    "if torch.cuda.is_available():  \n",
    "    Q = Q.cuda()\n",
    "    P = P.cuda()\n",
    "    D_cat = D_gauss.cuda()\n",
    "    D_gauss = D_net_gauss().cuda()\n",
    "# Set learning rates\n",
    "gen_lr, reg_lr = 0.0006, 0.0008  \n",
    "# Set optimizators\n",
    "P_decoder = optim.Adam(P.parameters(), lr=gen_lr)  \n",
    "Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)  \n",
    "Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)  \n",
    "D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     27,
     32,
     42,
     74,
     156
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "# Load data and create Data loaders\n",
    "##################################\n",
    "def load_data(data_path='../data/'):\n",
    "    print('loading data!')\n",
    "    trainset_labeled = pickle.load(open(data_path + \"train_labeled.p\", \"rb\"))\n",
    "    trainset_unlabeled = pickle.load(open(data_path + \"train_unlabeled.p\", \"rb\"))\n",
    "    # Set -1 as labels for unlabeled data\n",
    "    trainset_unlabeled.train_labels = torch.from_numpy(np.array([-1] * 47000))\n",
    "    validset = pickle.load(open(data_path + \"validation.p\", \"rb\"))\n",
    "\n",
    "    train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled,\n",
    "                                                       batch_size=train_batch_size,\n",
    "                                                       shuffle=True, **kwargs)\n",
    "\n",
    "    train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled,\n",
    "                                                         batch_size=train_batch_size,\n",
    "                                                         shuffle=True, **kwargs)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(validset, batch_size=valid_batch_size, shuffle=True)\n",
    "\n",
    "    return train_labeled_loader, train_unlabeled_loader, valid_loader\n",
    "\n",
    "\n",
    "####################\n",
    "# Utility functions\n",
    "####################\n",
    "def save_model(model, filename):\n",
    "    print('Best model so far, saving it...')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def report_loss(epoch, D_loss_gauss, G_loss, recon_loss):\n",
    "    '''\n",
    "    Print loss\n",
    "    '''\n",
    "    print('Epoch-{}; D_loss_gauss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'.format(epoch,\n",
    "                                                                                   D_loss_gauss.data[0],\n",
    "                                                                                   G_loss.data[0],\n",
    "                                                                                   recon_loss.data[0]))\n",
    "\n",
    "\n",
    "def create_latent(Q, loader):\n",
    "    '''\n",
    "    Creates the latent representation for the samples in loader\n",
    "    return:\n",
    "        z_values: numpy array with the latent representations\n",
    "        labels: the labels corresponding to the latent representations\n",
    "    '''\n",
    "    Q.eval()\n",
    "    labels = []\n",
    "\n",
    "    for batch_idx, (X, target) in enumerate(loader):\n",
    "\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        # X.resize_(loader.batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        labels.extend(target.data.tolist())\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "        # Reconstruction phase\n",
    "        z_sample = Q(X)\n",
    "        if batch_idx > 0:\n",
    "            z_values = np.concatenate((z_values, np.array(z_sample.data.tolist())))\n",
    "        else:\n",
    "            z_values = np.array(z_sample.data.tolist())\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return z_values, labels\n",
    "\n",
    "\n",
    "####################\n",
    "# Train procedure\n",
    "####################\n",
    "def train(P, Q, D_gauss, P_decoder, Q_encoder, Q_generator, D_gauss_solver, data_loader):\n",
    "    '''\n",
    "    Train procedure for one epoch.\n",
    "    '''\n",
    "    TINY = 1e-15\n",
    "    # Set the networks in train mode (apply dropout when needed)\n",
    "    Q.train()\n",
    "    P.train()\n",
    "    D_gauss.train()\n",
    "\n",
    "    # Loop through the labeled and unlabeled dataset getting one batch of samples from each\n",
    "    # The batch size has to be a divisor of the size of the dataset or it will return\n",
    "    # invalid samples\n",
    "    for X, target in data_loader:\n",
    "\n",
    "        # Load batch and normalize samples to be between 0 and 1\n",
    "        X = X * 0.3081 + 0.1307\n",
    "        X.resize_(train_batch_size, X_dim)\n",
    "        X, target = Variable(X), Variable(target)\n",
    "        if cuda:\n",
    "            X, target = X.cuda(), target.cuda()\n",
    "\n",
    "        # Init gradients\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "        #######################\n",
    "        # Reconstruction phase\n",
    "        #######################\n",
    "        z_sample = Q(X)\n",
    "        X_sample = P(z_sample)\n",
    "        recon_loss = F.binary_cross_entropy(X_sample + TINY, X.resize(train_batch_size, X_dim) + TINY)\n",
    "\n",
    "        recon_loss.backward()\n",
    "        P_decoder.step()\n",
    "        Q_encoder.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "        #######################\n",
    "        # Regularization phase\n",
    "        #######################\n",
    "        # Discriminator\n",
    "        Q.eval()\n",
    "        z_real_gauss = Variable(torch.randn(train_batch_size, z_dim) * 5.)\n",
    "        if cuda:\n",
    "            z_real_gauss = z_real_gauss.cuda()\n",
    "\n",
    "        z_fake_gauss = Q(X)\n",
    "\n",
    "        D_real_gauss = D_gauss(z_real_gauss)\n",
    "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + TINY) + torch.log(1 - D_fake_gauss + TINY))\n",
    "\n",
    "        D_loss.backward()\n",
    "        D_gauss_solver.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "        # Generator\n",
    "        Q.train()\n",
    "        z_fake_gauss = Q(X)\n",
    "\n",
    "        D_fake_gauss = D_gauss(z_fake_gauss)\n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + TINY))\n",
    "\n",
    "        G_loss.backward()\n",
    "        Q_generator.step()\n",
    "\n",
    "        P.zero_grad()\n",
    "        Q.zero_grad()\n",
    "        D_gauss.zero_grad()\n",
    "\n",
    "    return D_loss, G_loss, recon_loss\n",
    "\n",
    "\n",
    "def generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader):\n",
    "    torch.manual_seed(10)\n",
    "\n",
    "    if cuda:\n",
    "        Q = Q_net().cuda()\n",
    "        P = P_net().cuda()\n",
    "        D_gauss = D_net_gauss().cuda()\n",
    "    else:\n",
    "        Q = Q_net()\n",
    "        P = P_net()\n",
    "        D_gauss = D_net_gauss()\n",
    "\n",
    "    # Set learning rates\n",
    "    gen_lr = 0.0001\n",
    "    reg_lr = 0.00005\n",
    "\n",
    "    # Set optimizators\n",
    "    P_decoder = optim.Adam(P.parameters(), lr=gen_lr)\n",
    "    Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)\n",
    "\n",
    "    Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)\n",
    "    D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        D_loss_gauss, G_loss, recon_loss = train(P, Q, D_gauss, P_decoder, Q_encoder,\n",
    "                                                 Q_generator,\n",
    "                                                 D_gauss_solver,\n",
    "                                                 train_unlabeled_loader)\n",
    "        if epoch % 10 == 0:\n",
    "            report_loss(epoch, D_loss_gauss, G_loss, recon_loss)\n",
    "\n",
    "    return Q, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/train_labeled.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fc3973b000c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_unlabeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_unlabeled_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b44d1459a4c4>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading data!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrainset_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train_labeled.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtrainset_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train_unlabeled.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Set -1 as labels for unlabeled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/train_labeled.p'"
     ]
    }
   ],
   "source": [
    "train_labeled_loader, train_unlabeled_loader, valid_loader = load_data()\n",
    "Q, P = generate_model(train_labeled_loader, train_unlabeled_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
